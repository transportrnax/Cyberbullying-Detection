# Cyberbullying Detection

Detecting cyberbullying from short social text using classic ML and simple neural models, with robust preprocessing and adversarial stress-tests.

## ðŸ” Overview

* **Goal:** Binary classify messages as **Aggressive** vs **Non-Aggressive**.
* **Use cases:** Content moderation, campus/community monitoring, early warning.
* **Whatâ€™s special:** Multiple text representations (TF-IDF, Word2Vec, log-count weighting), side-by-side model baselines, and **adversarial checks** on sensitive keywords.

## ðŸ“¦ Dataset

* **Source:** Two CSV files: `Aggressive_All.csv` and `Non_Aggressive_All.csv`.
* **Size:** â‰ˆ118k rows each.
* **Columns:** `No.`, `Message`.
* **Notes:** The â€œAggressiveâ€ file mixes explicit insults, biased/hostile opinions, and noise; the â€œNon-Aggressiveâ€ file contains normal statements but also many ambiguous tokens.

> Please add the actual download link and licensing terms here if you publish the data.

## ðŸ§¹ Preprocessing & Features

* Cleaning: lowercasing, punctuation & digit removal, stop-word filtering, stemming/tokenization.
* Representations:

  1. **TF-IDF** (optionally **PCA** to reduce dimension) â†’ scaling.
  2. **Word2Vec** average word vectors â†’ scaling.
  3. **NaÃ¯ve Bayes log-count weighting** (log-count ratio per class).
* Split: **80/20** train/test with cross-validation; compare with/without standardization and information gain.

## ðŸ¤– Models

* **Classical:** SVM, Random Forest, Logistic Regression (mainly on TF-IDF).
* **Neural (simple):** MLP for vector features (e.g., 128-Dropout-128-Dropout-Softmax).
* **Experimental:** Deep feedforward (128â†’64â†’1, ReLU/Sigmoid).
* **Error handling:** Adversarial tests with sensitive words (e.g., â€œfuck / shit / sucksâ€) embedded in neutral/positive context to reduce keyword-triggered false positives.

> If you later use BERT/Transformers or other embeddings, document it here.

## ðŸ“ˆ Evaluation

* Metrics: Accuracy, Precision, Recall, F1; Confusion Matrix.
* **Subset testing** on sensitive-keyword samples to check over-triggering.
* (Optional) AUC and PR curves.

## ðŸ§ª Adversarial Checks (Sensitive Words)

* Build neutral/positive sentences that contain sensitive tokens and verify the model doesnâ€™t blindly flag them.
* If it still over-fires:

  * Add such samples to training (data balancing/weighting).
  * Switch to context-aware encoders (e.g., BERT) or use class-/sample-weighted loss.

## ðŸ”­ Future Work

* Transformer encoders with attention.
* Hierarchical/typed labels for bullying categories.
* Hard-example mining and data augmentation (EDA, back-translation).

If you share your actual script names/args and metric tables, Iâ€™ll tailor this to your repo exactly.
