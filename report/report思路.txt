æˆ‘ä»¬çš„æ•°æ®æ¥æºï¼šEjaz, Naveed; Choudhury, Salimur; Razi, Fakhra (2024), â€œA Comprehensive Dataset for Automated Cyberbullying Detectionâ€, Mendeley Data, V2, doi: 10.17632/wmx9jj2htd.2ã€‚ç½‘ç»œæ¬ºå‡Œçš„ç‰¹ç‚¹æ˜¯åŒä¼´ä¹‹é—´å…·æœ‰æ”»å‡»æ€§ã€é‡å¤æ€§å’Œæ•…æ„çš„äº¤æµã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æ•°æ®é›†åªå…³æ³¨æ”»å‡»æ€§æ–‡æœ¬ï¼ˆaggressive textï¼‰ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºæ”»å‡»æ€§æˆ–éæ”»å‡»æ€§ï¼Œè€Œå¿½ç•¥äº†ç½‘ç»œæ¬ºå‡Œçš„å…¶ä»–ä¸‰ä¸ªæ–¹é¢ repetition, peer behavior, and intent to harmã€‚è¯¥æ•°æ®é›†æ˜¯ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œæ˜¯ç”±è¿™å››ä¸ªç‰¹å¾ç”Ÿæˆçš„ã€‚

æ”»å‡»æ€§æ–‡æœ¬ã€é‡å¤ã€åŒè¾ˆæ€§å’Œä¼¤å®³æ„å›¾çš„æ•°æ®é›†

æ¯ä¸ªæ•°æ®é›†åŒ…å«ä¸¤åˆ—â€œNo.â€å’Œâ€œMessageâ€ï¼Œä¸¤è€…çš„æ•°æ®é‡å‡ä¸º118829è¡Œï¼Œè¡¨æ˜è¿™ä¸¤ç§æ•°æ®ä¹‹é—´åˆ†å¸ƒå‡è¡¡ã€‚æˆ‘ä»¬æŠŠæ•°æ®é›†åˆ†å‰²ä¸º70%çš„è®­ç»ƒé›†ï¼Œ15%çš„æµ‹è¯•é›†ï¼Œ15%çš„éªŒè¯é›†ã€‚
åœ¨æ”»å‡»æ€§æ•°æ®ä¸­ï¼Œæœ‰40%çš„æ”»å‡»æ€§è¯­è¨€ï¼Œ30%çš„å¯¹æ•æ„Ÿè¯é¢˜ï¼ˆç§æ—ã€å®—æ•™ã€æ”¿æ²»ï¼‰çš„ç‰‡é¢çœ‹æ³•ï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ— æ„ä¹‰çš„è¡¨è¾¾ã€‚
åœ¨éæ”»å‡»æ€§æ•°æ®ä¸­ï¼Œå®ƒåŒ…å«ä¸å…·æ”»å‡»æ€§çš„æ™®é€šè¯„è®ºã€‚ç„¶è€Œï¼Œå®ƒä¹ŸåŒ…å«äº†è®¸å¤šæ„å›¾ä¸æ˜ç¡®çš„è¯è¯­ã€‚

æ•°æ®å¤„ç†ï¼šDigit removalï¼ŒStop words filteringï¼šlike â€œis, the, aï¼ŒRemove punctuationï¼ŒStemming: as it can save more semantic informationï¼ŒWord segmentation: to cut sentence into word

Methodologyï¼š
1.é¦–å…ˆæ˜¯ç”¨äº†TFIDFå’Œä¿¡æ¯å¢ç›Šï¼Œå¾—åˆ°ç‰¹å¾çŸ©é˜µï¼ŒPCAé™ç»´ï¼Œ+ normalizationã€‚æ™®é€šæœºå™¨å­¦ä¹ modelï¼šSVM + RF + LR

2.TFIDFï¼Œå®ƒæ³¨é‡çš„æ˜¯è¯å‡ºç°çš„é¢‘ç‡ï¼Œè€Œä¸æ˜¯è¯­ä¹‰ã€‚æ‰€ä»¥æ”¹è¿›äº†ç”¨å¹³å‡è¯å‘é‡ã€‚Work2Vecæå–çš„ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œç„¶åç”¨äº†MLPæ¥è®­ç»ƒ
	1) Input Layer
	2) First Hidden Layer with 128 neurons and ReLU activation
	3) Dropout Layer with a rate of 0.2
	4) Second Hidden Layer with 128 neurons and ReLU activation
	5) Dropout Layer with a rate of 0.5
	6) Output Layer with 2 neurons and Softmax activationIn total
Traditional Sentence Representation, simple averaging techniques where each word in the sentence has equal weight.
Eg: Support a sentence contains N words , and word vectors are ğ‘£_1,ğ‘£_2,â€¦.,ğ‘£_ğ‘›   ,
then the  traditional average word vector s of the sentence is :ğ‘ =1/ğ‘ âˆ‘24_(ğ‘–=1)^ğ‘â–’ğ‘£_ğ‘– 
Problem: Simple averaging is sensitive to random outliers and extreme values.
Limitationï¼šEqual weighting can reduce performance

3.å¦‚æœå•çº¯ç”¨å¹³å‡è¯å‘é‡çš„è¯ï¼Œå®ƒæ²¡æœ‰åŠæ³•è¡¨ç¤ºä¸€äº›é‡è¦çš„å•è¯åœ¨å¥å­ä¸­æ¯”è¾ƒé‡è¦çš„åœ°ä½ã€‚å¦‚æœå½“å‡ºç°ä¸€äº›å¼‚å¸¸å€¼æˆ–è€…è¯´ä¸€äº›å¥‡æ€ªçš„å•è¯çš„æ—¶å€™ï¼Œæ•´å¥è¯å°±ä¼šè¢«å¸¦åã€‚æ‰€ä»¥ä¸ºäº†ç”¨åé‡è¯­ä¹‰ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨Naive byseåŠ æƒå¹³å‡è¯å‘é‡æ¥ç”Ÿæˆè¯å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç”¨æ·±åº¦å‰é¦ˆæ¨¡å‹
åŸºæœ¬æ€è·¯ï¼š
è®¡ç®—ç‰¹å®šç±»åˆ«ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼Œç„¶åæ ¹æ®è¿™äº›é¢‘ç‡çš„æ¯”ç‡æ¥è¡¡é‡è¯¥å•è¯åœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡è®¡ç®—æ¯ä¸ªå•è¯çš„å¯¹æ•°è®¡æ•°æ¯”ç‡ï¼Œä¸ºæ¯ä¸ªå•è¯èµ‹äºˆä¸€ä¸ªæƒé‡ã€‚
Step1â€”Generated word vector 
Tool: Pre-trained word embedding models like Word2Vec.
Why Word2Vec?
Skip-gram model learns semantic relationships based on context and can adjust dynamically based on surrounding words.
Step2â€”Word Frequency Weight Calculation :
Compute word frequency in training data
Calculate word frequency per class 
Compute the probability for each word given a class
Apply Laplace Smoothing (Add 1 smoothing) to avoid zero probabilities.
Step3â€”Calculating Log-Count Ratios
Calculate the log ratio between two classes for each wordï¼š(here is a formula)
It measures the difference in the distribution of the word w in the two categories
Step 4: Weighted Average
Formula: For each word vector  ğ’—_ğ’˜ in the sentence, apply its corresponding weight ğ’˜ and compute the weighted average
Model Select: Deep Feedforward Neural Networks
Advantage : With their multi-layered structure, can capture complex nonlinear relationships and patterns.
Layer structureï¼š
Layer 1ï¼šfully connected layer with 128 neurons + ReLU 
layer 2ï¼š fully connected layer with 64 neurons +ReLU 
Layer 3 ï¼šSingle neuron with sigmoid activation
4.åˆ°è¿™ä¸€æ­¥ä¸ºæ­¢å‡†ç¡®ç‡å¾ˆé«˜ï¼Œè¾¾åˆ°äº†99%ï¼Œä½†æ˜¯ï¼Œåœ¨å‡†ç¡®ç‡é«˜çš„åŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°ä»–å¯¹äºä¸€äº›æç«¯è¯éå¸¸æ•æ„Ÿï¼Œæ¯”å¦‚è¯´fuckæˆ–è¯´stupidè¿™ç§è´Ÿé¢æ€§éå¸¸å¼ºçš„è¿™ç§è¯çš„è¯ï¼Œå®ƒè¿˜æ˜¯ä¼šæ£€æµ‹ä¸ºæ”»å‡»æ€§æ–‡æœ¬ï¼ŒFor example, â€˜Youâ€™re so fucking beautifulâ€™ was wrongly labeled as aggressive because of the f-word, even though the overall meaning is positive. ä»–è¿˜ä¸èƒ½åŒºåˆ†è¿™æ ·çš„ä¾‹å­ã€‚æˆ‘ä»¬åé¢çš„å¾®è°ƒå’Œæ”¹è¿›ç”¨äº†å¯¹æŠ—è®­ç»ƒï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¯¹æŠ—æ€§æµ‹è¯•ã€‚æˆ‘ä»¬æ‰‹åŠ¨ç”Ÿæˆäº†è¿™äº›æ•æ„Ÿè¯å‡ºç°åœ¨æ­£é¢æˆ–ä¸­æ€§è¯­å¢ƒä¸­çš„æ ·æœ¬ï¼Œä¾‹å¦‚å›¾ç‰‡ä¸­ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ã€‚å¹¶ä¸”å¼•å…¥äº†Nbeatsï¼Œæ ¹æ®è¯­ä¹‰åŠ¨æ€çš„è°ƒæ•´è¯å‘é‡ï¼Œè¿™æ ·å› ä¸ºä¸åŒæ„æ€ï¼Œæ‰€ä»¥è¯´å®ƒå°±ä¼šæœ‰ä¸åŒçš„è¯å‘é‡ã€‚ç„¶åå†ç”¨transformerçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚å»è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥æ›´è´´è¿‘è¯­ä¹‰å»åˆ¤æ–­è¿™åˆ°åº•æ˜¯æ”»å‡»æ€§è¿˜æ˜¯éæ”»å‡»æ€§æ–‡æœ¬ï¼Œè€Œä¸æ˜¯ç®€å•çš„æ–‡æœ¬åˆ¤æ–­ã€‚

EXPERIMENTAL RESULTSï¼š
æˆ‘ä»¬ä½¿ç”¨é¢„æµ‹ç²¾åº¦ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹æ•°æ®é›†çš„æ¨¡å‹åˆ†ç±»æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬æ”¹è¿›å’Œæå‡åçš„çš„ç»“æœä¸ä¼ ç»Ÿçš„æ–‡æœ¬è¡¨ç¤ºå’Œåˆ†ç±»æŠ€æœ¯ï¼ˆå¦‚TF-IDFå’ŒNBSVMï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†Nbeatsè´´è¿‘è¯­ä¹‰å»å¢å¼ºåˆ†ç±»ç»“æœã€‚
è¯„ä¼°ç»“æœ
æˆ‘ä»¬ä½¿ç”¨äº†ä¸€å¥—å…¨é¢çš„ç»©æ•ˆè¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€å¬å›ç‡ã€lossç‡ã€‚
å¦‚è¡¨Iæ‰€ç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨é€šç”¨çš„é¢„è®­ç»ƒWord2Vecæ¨¡å‹æ¥ç”Ÿæˆè¯å‘é‡ï¼Œå¹¶ä½¿ç”¨MLPæ¥è®­ç»ƒé¢„æµ‹ç±»åˆ«ï¼Œè¿˜æœ‰ä½¿ç”¨Naive byseåŠ æƒå¹³å‡è¯å‘é‡æ¥ç”Ÿæˆè¯å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç”¨æ·±åº¦å‰é¦ˆæ¥è®­ç»ƒé¢„æµ‹ç±»åˆ«ï¼Œè¿˜æœ‰ä¸€ç§ç±»å‹æ˜¯å¼•å…¥äº†å¯¹æŠ—è®­ç»ƒå’ŒNbeatsã€‚è®¡ç®—åˆ†ç±»ç²¾åº¦ï¼Œä»è€Œè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚
å®éªŒçš„åˆ†ç±»è¡¨æ˜ï¼Œæ™®é€šå¹³å‡å€¼è®¡ç®—å¥å­å‘é‡çš„ä¼ ç»ŸæŠ€æœ¯ç›¸æ¯”ï¼Œä½¿ç”¨Naive byseåŠ æƒå¹³å‡è¯å‘é‡å’Œæ·±åº¦å‰é¦ˆå¯ä»¥æé«˜å‡†ç¡®æ€§
â€¢ä½¿ç”¨word2Vecï¼ˆ86%ï¼‰æ¯”ä½¿ç”¨ä¼ ç»ŸTF-IDFï¼ˆ79%ï¼‰æ—¶ï¼Œæ•°æ®é›†çš„æ€»ä½“å‡†ç¡®æ€§æé«˜äº†7%
â€¢ä½¿ç”¨Naive byseåŠ æƒå¹³å‡è¯å‘é‡ï¼ˆ99%ï¼‰æ¯”ä½¿ç”¨ä¼ ç»Ÿword2Vecï¼ˆ86%ï¼‰æ—¶ï¼Œæ•°æ®é›†çš„æ€»ä½“å‡†ç¡®æ€§æé«˜äº†13%
â€¢åœ¨Naive byseåŠ æƒå¹³å‡è¯å‘é‡æ¨¡å‹é‡Œå¼•å…¥Nbeatsåï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡ç•¥æœ‰ä¸‹é™
â€¢ä¸‰ä¸ªæ¨¡å‹çš„æ€»ä½“åˆ†ç±»é¢„æµ‹å‡†ç¡®ç‡æœ€é«˜çš„ç»“æœæ˜¯ä½¿ç”¨æˆ‘ä»¬çš„Naive byseæ¨¡å‹ï¼Œå‡†ç¡®ç‡çº¦ä¸º99%
ä¸ä¼ ç»ŸæŠ€æœ¯æ¯”è¾ƒ
æˆ‘ä»¬æ£€æµ‹å¯¹äºå«æœ‰æ•æ„Ÿè¯æ±‡ä½†æ•´ä½“è¯­ä¹‰ä¸ºç§¯æçš„å¥å­è¿›è¡Œåˆ¤æ–­ã€‚
â€¢ä½¿ç”¨ä¼ ç»ŸTF-IDFï¼Œword2Vecå’Œæ²¡æœ‰å¼•ç”¨Nbeatsçš„æ¨¡å‹é‡Œï¼Œå¯¹äºå«æœ‰æ•æ„Ÿè¯æ±‡ä½†æ•´ä½“è¯­ä¹‰ä¸ºç§¯æçš„å¥å­çš„åˆ¤æ–­æ”»å‡»æ€§é«˜è¾¾99%
â€¢åœ¨Naive byseåŠ æƒå¹³å‡è¯å‘é‡æ¨¡å‹é‡Œå¼•å…¥Nbeatsåï¼Œå¯¹äºå«æœ‰æ•æ„Ÿè¯æ±‡ä½†æ•´ä½“è¯­ä¹‰ä¸ºç§¯æçš„å¥å­è¯†åˆ«ç‡æ˜¾è‘—æé«˜ã€‚ç”±åŸæ¥çš„99%çš„æ”»å‡»æ€§ä¸‹é™åˆ°0.06%


å‚è€ƒæˆ‘ç»™ä½ çš„ä»£ç ï¼Œé‡æ–°å†™reportçš„Methodologyéƒ¨åˆ†ã€‚è¦ä»‹ç»1.æˆ‘ä»¬ä½¿ç”¨çš„ BERTæ¨¡å‹ï¼Œæ¨¡å‹ç»“æ„ï¼Œé¢„è®­ç»ƒæƒé‡ï¼Œè¾“å…¥æ ¼å¼ 2.ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ï¼Œä¼˜åŒ–å™¨çš„å‚æ•°è®¾ç½®ï¼Œä½œç”¨ 3.æŸå¤±å‡½æ•° CrossEntropyLoss 4. è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯ 5.æ—©åœæœºåˆ¶ 6. k-foldäº¤å‰éªŒè¯ 7. beatmodelçš„è¶…å‚æ•°è¶…å‚æ•° 8.mlpçš„å…¨è¿æ¥å±‚ã€‚ç€é‡ä»‹ç»æˆ‘ä»¬çš„æ¨¡å‹ ï¼Œä½“ç°æˆ‘ä»¬ä½¿ç”¨äº†ä¸åŒç½‘ç»œä¸åŒæ¨¡å‹å»æ”¹è¿›å‡†ç¡®ç‡çš„è¿™ä¸ªè¿‡ç¨‹

\textbf{MLP Architecture:}
\begin{itemize}
    \item Input Layer: Takes Word2Vec embeddings of sentences.
    \item First Hidden Layer: 128 neurons, ReLU activation, dropout rate = 0.2.
    \item Second Hidden Layer: 128 neurons, ReLU activation, dropout rate = 0.5.
    \item Output Layer: 2 neurons, Softmax activation.
\end{itemize}
\textbf{DNN Architecture:}
\begin{itemize}
    \item Layer 1: Fully connected, 128 neurons, ReLU activation.
    \item Layer 2: Fully connected, 64 neurons, ReLU activation.
    \item Layer 3: Single neuron, Sigmoid activation.
\end{itemize}

SVMï¼š
               precision    recall  f1-score   support

           0       0.76      0.91      0.83     13561
           1       0.88      0.68      0.77     12211

    accuracy                           0.80     25772
   macro avg       0.82      0.80      0.80     25772
weighted avg       0.82      0.80      0.80     25772

LRï¼š
              precision    recall  f1-score   support

           0       0.77      0.90      0.83     13845
           1       0.86      0.68      0.76     11926

    accuracy                           0.80     25771
   macro avg       0.81      0.79      0.79     25771
weighted avg       0.81      0.80      0.80     25771

RFï¼š
              precision    recall  f1-score   support

           0       0.76      0.90      0.83     13592
           1       0.86      0.69      0.77     12179
...
    accuracy                           0.89     25771
   macro avg       0.89      0.89      0.89     25771
weighted avg       0.89      0.89      0.89     25771

word2Vec+NBLCN+mlp:
Average Accuracy: 0.8563289710340245
Classification Report (from each fold):

Fold 1 Classification Report:
              precision    recall  f1-score   support

           0       0.86      0.86      0.86      2674
           1       0.84      0.84      0.84      2345

    accuracy                           0.85      5019
   macro avg       0.85      0.85      0.85      5019
weighted avg       0.85      0.85      0.85      5019


Fold 2 Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.85      0.86      2698
           1       0.83      0.86      0.84      2321
...
    accuracy                           0.86      5018
   macro avg       0.86      0.86      0.86      5018
weighted avg       0.86      0.86      0.86      5018

Beat(BERT):
accuracy: 0.97

 CONCLUSION AND FUTURE WORK
 A. Research Achievements Summary
 B. Research Limitations
 C. Future Research Directions
 