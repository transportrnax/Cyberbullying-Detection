{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f32fb5150df5248"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data processing\n",
   "id": "3845a04af1d7a8eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:48:34.596049Z",
     "start_time": "2024-12-17T08:48:24.548021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the preprocessed CSV file\n",
    "data = pd.read_csv(\"data_preprocessed_gen.csv\")\n",
    "texts = data[\"text\"]\n",
    "labels = data[\"label\"]\n",
    "\n",
    "\n",
    "# Check for missing values in 'text' column\n",
    "print(data['text'].isnull().sum())\n",
    "\n",
    "# Remove rows with NaN values in 'text' column\n",
    "data = data.dropna(subset=['text'])\n",
    "\n",
    "# Tokenizing the text and adding token count column\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data['token_count'] = data['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# Print token count statistics\n",
    "print(data['token_count'].describe())\n",
    "\n",
    "# Check for empty or whitespace-only texts\n",
    "empty_or_whitespace = data[data['text'].str.strip() == '']\n",
    "print(f\"Number of empty or whitespace-only rows: {len(empty_or_whitespace)}\")\n",
    "\n",
    "# Remove empty or whitespace-only rows\n",
    "data = data[data['text'].str.strip() != '']\n",
    "\n",
    "# Check the character length of each text\n",
    "data['char_length'] = data['text'].apply(len)\n",
    "print(data['char_length'].describe())\n",
    "\n",
    "# Remove texts that are longer than 512 characters\n",
    "data = data[data['char_length'] <= 512]\n"
   ],
   "id": "6b62c548853222f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    29996.000000\n",
      "mean        31.162988\n",
      "std         96.434334\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%         13.000000\n",
      "75%         26.000000\n",
      "max       3626.000000\n",
      "Name: token_count, dtype: float64\n",
      "Number of empty or whitespace-only rows: 0\n",
      "count    29996.000000\n",
      "mean       154.718562\n",
      "std        434.866065\n",
      "min          1.000000\n",
      "25%         27.000000\n",
      "50%         52.500000\n",
      "75%        125.000000\n",
      "max      11331.000000\n",
      "Name: char_length, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:49:00.607954Z",
     "start_time": "2024-12-17T08:49:00.583934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = [str(text) if not pd.isnull(text) else '' for text in texts]\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            # Tokenizing the text\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "# Ensure data has no missing or non-string values\n",
    "data['text'] = data['text'].fillna('').astype(str)\n",
    "texts = data['text'].tolist()\n",
    "data['label'] = data['label'].fillna(0).astype(int)\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# Create Dataset\n",
    "max_len = 64\n",
    "dataset = TextDataset(texts, labels, tokenizer, max_len)\n",
    "\n",
    "# Check a sample from the dataset\n",
    "sample = dataset[0]\n",
    "print(\"Input IDs shape:\", sample['input_ids'].shape)\n",
    "print(\"Attention Mask shape:\", sample['attention_mask'].shape)\n",
    "print(\"Label:\", sample['label'])\n"
   ],
   "id": "4b016976e1154c9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([64])\n",
      "Attention Mask shape: torch.Size([64])\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### partition dataset\n",
   "id": "62476f2ba049f3da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:11:04.479923Z",
     "start_time": "2024-12-17T17:11:04.448561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n"
   ],
   "id": "13c0b1a76b3a10ad",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:11:06.068359Z",
     "start_time": "2024-12-17T17:11:06.012228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create DataLoader for both training and validation\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Check the output of a batch from the DataLoader\n",
    "for batch in train_loader:\n",
    "    print(\"Batch input_ids shape:\", batch['input_ids'].shape)  # (batch_size, max_len)\n",
    "    print(\"Batch attention_mask shape:\", batch['attention_mask'].shape)  # (batch_size, max_len)\n",
    "    print(\"Batch labels shape:\", batch['label'].shape)  # (batch_size,)\n",
    "    break\n"
   ],
   "id": "add961b4d4687bc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids shape: torch.Size([32, 64])\n",
      "Batch attention_mask shape: torch.Size([32, 64])\n",
      "Batch labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:49:08.970758Z",
     "start_time": "2024-12-17T08:49:07.332626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Load the pre-trained BERT model with classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "loss_fn = CrossEntropyLoss()\n"
   ],
   "id": "9abe748361881bcd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "F:\\anaconda\\envs\\cudaenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T16:59:28.657784Z",
     "start_time": "2024-12-17T15:58:14.749615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the training loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return total_loss / len(data_loader), correct_predictions.double() / len(data_loader.dataset)\n",
    "\n",
    "# Define the evaluation loop\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return total_loss / len(data_loader), correct_predictions.double() / len(data_loader.dataset)\n",
    "\n",
    "# Train and evaluate the model for multiple epochs\n",
    "epochs = 3\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    print(f\"Train loss: {train_loss}, accuracy: {train_acc}\")\n",
    "\n",
    "    val_loss, val_acc = eval_model(model, val_loader, loss_fn, device)\n",
    "    print(f\"Val loss: {val_loss}, accuracy: {val_acc}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc\n"
   ],
   "id": "7aa0e78c2bc5665",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 709/709 [18:39<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07151400246256792, accuracy: 0.9784467559943583\n",
      "Val loss: 0.19846900729881076, accuracy: 0.9247311827956989\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 709/709 [26:12<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07208224782806905, accuracy: 0.9775652327221438\n",
      "Val loss: 0.19846900729881076, accuracy: 0.9247311827956989\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 709/709 [11:31<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07203140167968691, accuracy: 0.9782704513399153\n",
      "Val loss: 0.19846900729881076, accuracy: 0.9247311827956989\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## text verification",
   "id": "9a88a660a373b614"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Tong\\AppData\\Local\\Temp\\ipykernel_36616\\4284443672.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_state.bin'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: good morning!\n",
      "Prediction: Not Offensive (Probability: 0.1037)\n",
      "Text: good idea!\n",
      "Prediction: Not Offensive (Probability: 0.0376)\n",
      "Text: you are stupid\n",
      "Prediction: Offensive (Probability: 0.9948)\n",
      "Text: fuck stupid\n",
      "Prediction: Offensive (Probability: 0.9957)\n",
      "Text: i love you\n",
      "Prediction: Not Offensive (Probability: 0.0229)\n"
     ]
    }
   ],
   "execution_count": 37,
   "source": [
    "# Load the best model\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Load the optimal model weights\n",
    "model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "model.eval()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prediction function\n",
    "def predict(texts, model, tokenizer, device):\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=128,  # Adjust max length as needed\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    return preds.cpu().numpy(), probabilities.cpu().numpy()\n",
    "\n",
    "# Example test sentences\n",
    "test_sentences = [\n",
    "    \"good morning!\",\n",
    "    \"good idea!\",\n",
    "    \"you are stupid\",\n",
    "    \"fuck stupid\",\n",
    "    \"i love you\",\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "predictions, probabilities = predict(test_sentences, model, tokenizer, device)\n",
    "\n",
    "# Print results\n",
    "for i, text in enumerate(test_sentences):\n",
    "    label = \"Offensive\" if predictions[i] == 1 else \"Not Offensive\"\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {label} (Probability: {probabilities[i][1]:.4f})\")\n"
   ],
   "id": "e61ff0a314cb2547"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
