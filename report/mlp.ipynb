{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:15:03.153702Z",
     "start_time": "2024-12-17T17:15:03.128896Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72fe5727de5ecd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:15:05.137325Z",
     "start_time": "2024-12-17T17:15:04.830288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the SpaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f13af0131dd0bd19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:15:09.208045Z",
     "start_time": "2024-12-17T17:15:09.179972Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove punctuation and numbers\n",
    "    text = ''.join([char if char.isalpha() or char.isspace() else ' ' for char in text])\n",
    "    # 3. Tokenization\n",
    "    doc = nlp(text)\n",
    "    # 4. Lemmatization and removal of stop words\n",
    "    tokens = [lemmatizer.lemmatize(token.text) for token in doc if token.text not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88f52b6edba50c",
   "metadata": {},
   "source": [
    "### Train the Word2Vec model using my own data ,which can make the vector i got more suit for my task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c31f8078da862ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:15:15.823176Z",
     "start_time": "2024-12-17T17:15:12.999277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data shape: (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the preprocessed data from the CSV file\n",
    "data = pd.read_csv(\"final_preprocessed_data.csv\")\n",
    "\n",
    "# Ensure all values in 'Message' are strings\n",
    "data['Message'] = data['Message'].fillna('')  # Fill NaN with empty strings\n",
    "data['Message'] = data['Message'].astype(str)  # Convert all to string\n",
    "\n",
    "# Filter out 20,000 samples with label 0 and 20,000 samples with label 1\n",
    "label_0_data = data[data['label'] == 0].head(20000)  # Filter for label = 0\n",
    "label_1_data = data[data['label'] == 1].head(20000)  # Filter for label = 1\n",
    "\n",
    "# Concatenate the two datasets to create the final training set\n",
    "train_data = pd.concat([label_0_data, label_1_data])\n",
    "\n",
    "# Print the shape of the final training data to ensure correct size\n",
    "print(f\"Final training data shape: {train_data.shape}\")\n",
    "\n",
    "# Extract the 'Message' column (assuming it's the text data column) and split each message into words\n",
    "sentences = train_data['Message'].apply(lambda x: x.split()).tolist()  # Split each sentence into a list of words\n",
    "\n",
    "# Import Word2Vec from Gensim for training the word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model using the processed sentences\n",
    "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained Word2Vec model for later use\n",
    "model.save(\"word2vec_final.model\")\n",
    "\n",
    "# The model can be loaded later using the following line:\n",
    "\n",
    "# model = Word2Vec.load\"word2vec_final.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866e3bace37fca0",
   "metadata": {},
   "source": [
    "# train my own word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c628acd45b1cfa",
   "metadata": {},
   "source": [
    "### Verify that my word2Vec model is valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6185a6688a51ca7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:15:22.820788Z",
     "start_time": "2024-12-17T17:15:22.539304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 0.9813039898872375), ('hunjan', 0.9581018686294556), ('kolkata', 0.9488511085510254), ('rfa', 0.9410589337348938), ('friendly', 0.9337308406829834)]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained Word2Vec model\n",
    "word2vec_model = Word2Vec.load(\"word2vec_final.model\")\n",
    "\n",
    "# Get the word vector for a specific word (e.g., 'hello')\n",
    "vector = word2vec_model.wv['hi']\n",
    "\n",
    "# Get the top 5 most similar words to 'hello'\n",
    "similar_words = word2vec_model.wv.most_similar('hi', topn=5)\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add06496748683b",
   "metadata": {},
   "source": [
    "## Calculate NBLCR term weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91640aaf2f0cc2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:16.596891Z",
     "start_time": "2024-12-17T17:23:16.549439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the preprocessed data from the CSV file\n",
    "data = pd.read_csv(\"data_preprocessed_val.csv\")\n",
    "# Ensure all values in 'Message' are strings\n",
    "data['text'] = data['text'].fillna('')  # Fill NaN with empty strings\n",
    "data['text'] = data['text'].astype(str)  # Convert all to string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "403677e5d4641eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:18.554053Z",
     "start_time": "2024-12-17T17:23:18.538270Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_class_word_frequencies(data, labels):\n",
    "    class_0_words = Counter()\n",
    "    class_1_words = Counter()\n",
    "    total_class_0_words = 0\n",
    "    total_class_1_words = 0\n",
    "    \n",
    "    for message, label in zip(data, labels):\n",
    "        words = message.split()  # Split the message into words\n",
    "        if label == 0:\n",
    "            class_0_words.update(words)\n",
    "            total_class_0_words += len(words)\n",
    "        else:\n",
    "            class_1_words.update(words)\n",
    "            total_class_1_words += len(words)\n",
    "    \n",
    "    return class_0_words, class_1_words, total_class_0_words, total_class_1_words\n",
    "\n",
    "# Now we can compute NBLCR weights\n",
    "def compute_nblcr_weights(class_0_words, class_1_words, total_class_0_words, total_class_1_words):\n",
    "    nblcr_weights = {}\n",
    "    \n",
    "    all_words = set(class_0_words.keys()).union(set(class_1_words.keys()))\n",
    "    \n",
    "    for word in all_words:\n",
    "        p_w_class_0 = class_0_words.get(word, 0) / total_class_0_words\n",
    "        p_w_class_1 = class_1_words.get(word, 0) / total_class_1_words\n",
    "        \n",
    "        # NBLCR weight: log ratio of probabilities\n",
    "        if p_w_class_1 > 0 and p_w_class_0 > 0:\n",
    "            nblcr_weight = np.log(p_w_class_1 / p_w_class_0)\n",
    "        else:\n",
    "            nblcr_weight = 0\n",
    "        \n",
    "        nblcr_weights[word] = nblcr_weight\n",
    "    \n",
    "    return nblcr_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306315bf77328fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e528789c963e2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:20.938044Z",
     "start_time": "2024-12-17T17:23:20.924798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>someone buggered stupid redirections   player ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well known many people many wikipedia account ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ignorant piece sht</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want masturbate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt natashaallen sexist male comedian much funn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25087</th>\n",
       "      <td>simpson cast member    please stop removing ed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25088</th>\n",
       "      <td>p case mistaken asked second opinion wikipedia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25089</th>\n",
       "      <td>thought pull back   mkr mkr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25090</th>\n",
       "      <td>let conduct talk talkcampbells soup can editor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25091</th>\n",
       "      <td>great finally look like long awaited project g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      someone buggered stupid redirections   player ...      1\n",
       "1      well known many people many wikipedia account ...      1\n",
       "2                                     ignorant piece sht      1\n",
       "3                                        want masturbate      1\n",
       "4      rt natashaallen sexist male comedian much funn...      1\n",
       "...                                                  ...    ...\n",
       "25087  simpson cast member    please stop removing ed...      0\n",
       "25088  p case mistaken asked second opinion wikipedia...      0\n",
       "25089                        thought pull back   mkr mkr      0\n",
       "25090  let conduct talk talkcampbells soup can editor...      0\n",
       "25091  great finally look like long awaited project g...      0\n",
       "\n",
       "[25092 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b53208875aec57a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:23.936330Z",
     "start_time": "2024-12-17T17:23:23.811235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract messages and labels\n",
    "messages = data['text']\n",
    "labels = data['label']\n",
    "\n",
    "# Compute word frequencies\n",
    "class_0_words, class_1_words, total_class_0_words, total_class_1_words = compute_class_word_frequencies(messages, labels)\n",
    "\n",
    "# Compute NBLCR weights for all words\n",
    "nblcr_weights = compute_nblcr_weights(class_0_words, class_1_words, total_class_0_words, total_class_1_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5309fb3f53cd4d",
   "metadata": {},
   "source": [
    "## Generating Sentence Vector Based on NBLCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c3e0b52565c1fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:25.712278Z",
     "start_time": "2024-12-17T17:23:25.698199Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_sentence_vector_nblcr(sentence, word2vec_model, nblcr_weights):\n",
    "    words = sentence.split()  # Split sentence into words\n",
    "    sentence_vector = np.zeros(word2vec_model.vector_size)  # Initialize sentence vector with zeros\n",
    "    total_weight = 0.0  # Initialize weight accumulator\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word2vec_model.wv:  # Check if the word exists in the Word2Vec model\n",
    "            weight = nblcr_weights.get(word, 0)  # Get NBLCR weight for the word (default is 0)\n",
    "            sentence_vector += weight * word2vec_model.wv[word]  # Add weighted word vector to sentence vector\n",
    "            total_weight += abs(weight)  # Accumulate weight\n",
    "    \n",
    "    # Normalize the sentence vector by the total weight if there are any weights\n",
    "    if total_weight > 0:\n",
    "        sentence_vector /= total_weight\n",
    "    \n",
    "    return sentence_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88717bf26b71244c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:32.462444Z",
     "start_time": "2024-12-17T17:23:30.570135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, we can compute sentence vectors for all messages\n",
    "sentence_vectors = np.array([\n",
    "    compute_sentence_vector_nblcr(text, word2vec_model, nblcr_weights) \n",
    "    for text in data['text']\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c2c8fda69f28297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:23:33.953265Z",
     "start_time": "2024-12-17T17:23:33.923587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined NBLCR weights have been saved to 'nblcr_weights_combined.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "#Save the merged nblcr_weights\n",
    "with open(\"nblcr_weights_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nblcr_weights, f)\n",
    "    print(\"Combined NBLCR weights have been saved to 'nblcr_weights_combined.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae075e72d3ac08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d107d008c1eb68",
   "metadata": {},
   "source": [
    "### The resulting Bayesian weighted average sentence vector is used as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e66245a70e0ecfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:24:05.667384Z",
     "start_time": "2024-12-17T17:24:05.654399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25092, 100)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed69750785ff00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d63d1445480739d7",
   "metadata": {},
   "source": [
    "### Building and training models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "770d14ab4ae9ac84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:24:07.443953Z",
     "start_time": "2024-12-17T17:24:07.401597Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get the labels\n",
    "y = data['label']  # Assuming 'label' column contains the labels\n",
    "\n",
    "# Initialize KFold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "# Define the neural network structure using Functional API\n",
    "input_layer = Input(shape=(100,))  # Assuming 100 features after processing\n",
    "x = Dense(256, activation='relu')(input_layer)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(2, activation='softmax')(x)  # 2 classes for binary classification\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),  # Use Adam optimizer\n",
    "              loss='sparse_categorical_crossentropy',  # Sparse categorical crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a219ce03a8ffdf9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:24:09.792878Z",
     "start_time": "2024-12-17T17:24:09.776933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m25,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,010</span> (230.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,010\u001b[0m (230.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,010</span> (230.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m59,010\u001b[0m (230.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "febbd593107a0022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:24:50.845892Z",
     "start_time": "2024-12-17T17:24:11.917212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step\n",
      "Average Accuracy: 0.8563289710340245\n",
      "Classification Report (from each fold):\n",
      "\n",
      "Fold 1 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      2674\n",
      "           1       0.84      0.84      0.84      2345\n",
      "\n",
      "    accuracy                           0.85      5019\n",
      "   macro avg       0.85      0.85      0.85      5019\n",
      "weighted avg       0.85      0.85      0.85      5019\n",
      "\n",
      "\n",
      "Fold 2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      2698\n",
      "           1       0.83      0.86      0.84      2321\n",
      "\n",
      "    accuracy                           0.85      5019\n",
      "   macro avg       0.85      0.85      0.85      5019\n",
      "weighted avg       0.85      0.85      0.85      5019\n",
      "\n",
      "\n",
      "Fold 3 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      2667\n",
      "           1       0.84      0.85      0.85      2351\n",
      "\n",
      "    accuracy                           0.85      5018\n",
      "   macro avg       0.85      0.85      0.85      5018\n",
      "weighted avg       0.86      0.85      0.85      5018\n",
      "\n",
      "\n",
      "Fold 4 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      2679\n",
      "           1       0.83      0.86      0.85      2339\n",
      "\n",
      "    accuracy                           0.86      5018\n",
      "   macro avg       0.86      0.86      0.86      5018\n",
      "weighted avg       0.86      0.86      0.86      5018\n",
      "\n",
      "\n",
      "Fold 5 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88      2749\n",
      "           1       0.84      0.86      0.85      2269\n",
      "\n",
      "    accuracy                           0.86      5018\n",
      "   macro avg       0.86      0.86      0.86      5018\n",
      "weighted avg       0.86      0.86      0.86      5018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to evaluate model using KFold\n",
    "def evaluate_model(model, X, y, kf):\n",
    "    accuracy_scores = []\n",
    "    classification_reports = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "        # Predict the classes\n",
    "        predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "        # Collect the metrics\n",
    "        accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "        classification_reports.append(classification_report(y_test, predictions))\n",
    "\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    avg_classification_report = classification_reports\n",
    "\n",
    "    return avg_accuracy, avg_classification_report\n",
    "\n",
    "# Evaluate the model using KFold cross-validation\n",
    "accuracy, report = evaluate_model(model, sentence_vectors, y, kf)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average Accuracy: {accuracy}\")\n",
    "print(\"Classification Report (from each fold):\")\n",
    "for idx, fold_report in enumerate(report):\n",
    "    print(f\"\\nFold {idx+1} Classification Report:\")\n",
    "    print(fold_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd1c447b58e77404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T17:25:04.109294Z",
     "start_time": "2024-12-17T17:25:04.084253Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'trained_model.h5'.\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('trained_model.h5')  # Save the model as a .h5 file\n",
    "print(\"Model saved as 'trained_model.h5'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
